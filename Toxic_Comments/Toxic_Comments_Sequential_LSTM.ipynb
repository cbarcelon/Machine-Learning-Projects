{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, AveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the input dataset\n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "#split the data into train and test sets\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#Replace all blank comments with text in training set\n",
    "#extract training comments \n",
    "comments_train = train[\"comment_text\"].fillna(\"cbarcelon\").values\n",
    "#extract the toxciity ratings\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_ratings = train[classes].values\n",
    "test_ratings = test[classes].values\n",
    "#extract test comments\n",
    "comments_test = test[\"comment_text\"].fillna(\"cbarcelon\").values\n",
    "\n",
    "#tokenizer the text\n",
    "#vectorize text\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(comments_train))\n",
    "tokenized_comments_train = tokenizer.texts_to_sequences(comments_train)\n",
    "tokenized_comments_test = tokenizer.texts_to_sequences(comments_test)\n",
    "#pad the text so each comment is uniform in length\n",
    "X_train = sequence.pad_sequences(tokenized_comments_train, maxlen= 50, truncating='post')\n",
    "X_test = sequence.pad_sequences(tokenized_comments_test, maxlen=50,  truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, None, 128)         640000    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 50)          71600     \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 50)          40400     \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, None, 50)          40400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 798,106\n",
      "Trainable params: 798,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define LSTM sequential model\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(5000, output_dim=128))\n",
    "lstm.add(Bidirectional(LSTM(50, return_sequences=True), merge_mode='ave'))\n",
    "lstm.add(Bidirectional(LSTM(50, return_sequences=True), merge_mode='ave'))\n",
    "lstm.add(Bidirectional(LSTM(50, return_sequences=True), merge_mode='ave'))\n",
    "lstm.add(GlobalMaxPool1D())\n",
    "lstm.add(Dropout(.5))\n",
    "lstm.add(Dense(100, activation='relu'))\n",
    "lstm.add(Dropout(.5))\n",
    "lstm.add(Dense(6, activation = \"sigmoid\"))\n",
    "\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#create checkpoint file\n",
    "file_path = \"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#early stop checkpoint\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
    "callbacks_list = [checkpoint, early] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69012 samples, validate on 7668 samples\n",
      "Epoch 1/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9434Epoch 00000: val_loss improved from inf to 0.14623, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 53s - loss: 0.2565 - acc: 0.9434 - val_loss: 0.1462 - val_acc: 0.9620\n",
      "Epoch 2/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9633Epoch 00001: val_loss improved from 0.14623 to 0.14553, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 51s - loss: 0.1552 - acc: 0.9633 - val_loss: 0.1455 - val_acc: 0.9620\n",
      "Epoch 3/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9645Epoch 00002: val_loss improved from 0.14553 to 0.08771, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 51s - loss: 0.1270 - acc: 0.9645 - val_loss: 0.0877 - val_acc: 0.9732\n",
      "Epoch 4/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9744Epoch 00003: val_loss improved from 0.08771 to 0.07328, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 51s - loss: 0.0832 - acc: 0.9744 - val_loss: 0.0733 - val_acc: 0.9760\n",
      "Epoch 5/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9779Epoch 00004: val_loss improved from 0.07328 to 0.06779, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 51s - loss: 0.0697 - acc: 0.9778 - val_loss: 0.0678 - val_acc: 0.9776\n",
      "Epoch 6/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9790Epoch 00005: val_loss improved from 0.06779 to 0.06549, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 51s - loss: 0.0643 - acc: 0.9790 - val_loss: 0.0655 - val_acc: 0.9785\n",
      "Epoch 7/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9801Epoch 00006: val_loss did not improve\n",
      "69012/69012 [==============================] - 51s - loss: 0.0596 - acc: 0.9801 - val_loss: 0.0676 - val_acc: 0.9783\n",
      "Epoch 8/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9806Epoch 00007: val_loss did not improve\n",
      "69012/69012 [==============================] - 51s - loss: 0.0562 - acc: 0.9806 - val_loss: 0.0679 - val_acc: 0.9772\n",
      "Epoch 9/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9814Epoch 00008: val_loss did not improve\n",
      "69012/69012 [==============================] - 51s - loss: 0.0531 - acc: 0.9814 - val_loss: 0.0678 - val_acc: 0.9770\n",
      "Epoch 10/10\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9820Epoch 00009: val_loss did not improve\n",
      "69012/69012 [==============================] - 51s - loss: 0.0511 - acc: 0.9820 - val_loss: 0.0670 - val_acc: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00b96dc7b8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train/fit the model\n",
    "lstm.fit(X_train, train_ratings, batch_size=750, epochs=10, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best weights\n",
    "lstm.load_weights(file_path)\n",
    "\n",
    "#make predictions on test set\n",
    "pred = lstm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log loss score function\n",
    "from sklearn.metrics import log_loss\n",
    "def calc_loss(y_true, y_pred):\n",
    "    return np.mean([log_loss(y_true[:, i], y_pred[:, i]) \n",
    "                    for i in range(y_true.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.065692556162\n"
     ]
    }
   ],
   "source": [
    "score = calc_loss(test_ratings, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
